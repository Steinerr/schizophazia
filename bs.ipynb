{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/denisivanov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import numpy\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import (\n",
    "    Dense,\n",
    "    LSTM, Dropout,\n",
    ")\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "TEXT_RESOURCE_PAGES = 10\n",
    "TEXT_CORPUS_PATH = 'text_corpus.txt'\n",
    "MODEL_WEIGHTS_PATH = 'model_weights_saved.hdf5'\n",
    "NEED_TO_LOAD_TEXT = False\n",
    "NEED_TO_FIT_MODEL = True\n",
    "SEQUENCE_LENGTH = 9\n",
    "EPOCH_NUM = 4\n",
    "BATCH_SIZE = 256\n",
    "NEURO_ISZE = 32"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def _get_domain(url):\n",
    "    return '.'.join(urlparse(url).netloc.split('.')[-2:])\n",
    "\n",
    "\n",
    "def _parse_default(html):\n",
    "    all_texts = ' '.join(re.findall(r'[а-яА-ЯёЁ]+', html))\n",
    "    all_texts = re.sub(r'\\n', r' ', all_texts)\n",
    "    all_texts = re.sub(r'\\s+', ' ', all_texts)\n",
    "    all_texts = all_texts.strip()\n",
    "    return all_texts\n",
    "\n",
    "\n",
    "def _parse_livejournal_page(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    all_texts = soup.find_all('p', {'class': None})\n",
    "    all_texts = ' '.join((p.text for p in all_texts))\n",
    "    return _parse_default(all_texts)\n",
    "\n",
    "\n",
    "page_parsers = {\n",
    "    'livejournal.com': _parse_livejournal_page,\n",
    "    'default': _parse_default,\n",
    "}\n",
    "\n",
    "def get_text_from_url(url) -> str:\n",
    "    print(f'Getting {url} ...')\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f'Some error: {response.status_code}')\n",
    "        return ''\n",
    "\n",
    "    parser = page_parsers.get(_get_domain(url))\n",
    "    if not parser:\n",
    "        parser = page_parsers['default']\n",
    "    return parser(response.text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "if NEED_TO_LOAD_TEXT:\n",
    "    urls = [f'https://pesen-net.livejournal.com/?skip={i*10}' for i in range(TEXT_RESOURCE_PAGES)]\n",
    "\n",
    "    texts = []\n",
    "    for url in urls:\n",
    "        text = get_text_from_url(url)\n",
    "        texts.append(text)\n",
    "\n",
    "    text = ' '.join(texts)\n",
    "    cleaned_text = text.lower()\n",
    "\n",
    "    with open(TEXT_CORPUS_PATH, 'w') as f:\n",
    "        f.write(cleaned_text)\n",
    "\n",
    "else:\n",
    "    with open(TEXT_CORPUS_PATH, 'r') as f:\n",
    "        cleaned_text = f.read()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(cleaned_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "cleaned_text = ' '.join(filter(lambda x: x not in stopwords.words('russian'), tokens))\n",
    "chars = sorted(list(set(cleaned_text)))\n",
    "char_to_num = {char: num for num, char in enumerate(chars)}\n",
    "num_to_char = {num: char for num, char in enumerate(chars)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "input_len = len(cleaned_text)\n",
    "vocab_len = len(chars)\n",
    "x_data, y_data = [], []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In seq:\tсамолёте \n",
      "Out seq:\tп\n",
      "X data:\t[[18, 1, 13, 15, 12, 33, 19, 6, 0]]\n",
      "Y data: \t[16]\n",
      "In seq:\tамолёте п\n",
      "Out seq:\tа\n",
      "X data:\t[[18, 1, 13, 15, 12, 33, 19, 6, 0], [1, 13, 15, 12, 33, 19, 6, 0, 16]]\n",
      "Y data: \t[16, 1]\n",
      "In seq:\tмолёте па\n",
      "Out seq:\tс\n",
      "X data:\t[[18, 1, 13, 15, 12, 33, 19, 6, 0], [1, 13, 15, 12, 33, 19, 6, 0, 16], [13, 15, 12, 33, 19, 6, 0, 16, 1]]\n",
      "Y data: \t[16, 1, 18]\n",
      "In seq:\tолёте пас\n",
      "Out seq:\tс\n",
      "X data:\t[[18, 1, 13, 15, 12, 33, 19, 6, 0], [1, 13, 15, 12, 33, 19, 6, 0, 16], [13, 15, 12, 33, 19, 6, 0, 16, 1], [15, 12, 33, 19, 6, 0, 16, 1, 18]]\n",
      "Y data: \t[16, 1, 18, 18]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, input_len - SEQUENCE_LENGTH, 1):\n",
    "    in_seq = cleaned_text[i:i + SEQUENCE_LENGTH]\n",
    "    out_seq = cleaned_text[i + SEQUENCE_LENGTH]\n",
    "\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])\n",
    "\n",
    "    if i <= 3:\n",
    "        print(\n",
    "            f'In seq:\\t{in_seq}',\n",
    "            f'Out seq:\\t{out_seq}',\n",
    "            f'X data:\\t{x_data}',\n",
    "            f'Y data: \\t{y_data}',\n",
    "            sep='\\n'\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If  you want to print something from `for`:\n",
    "\n",
    "```\n",
    "IOPub data rate exceeded.\n",
    "The notebook server will temporarily stop sending output\n",
    "to the client in order to avoid crashing it.\n",
    "To change this limit, set the config variable\n",
    "`--NotebookApp.iopub_data_rate_limit`.\n",
    "\n",
    "Current values:\n",
    "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
    "NotebookApp.rate_limit_window=3.0 (secs)\n",
    "```\n",
    "\n",
    "```\n",
    "To create a jupyter_notebook_config.py file, with all the defaults commented out, you can use the following command line:\n",
    "\n",
    "$ jupyter notebook --generate-config\n",
    "\n",
    "Open the file and search for c.NotebookApp.iopub_data_rate_limit\n",
    "\n",
    "Comment out the line c.NotebookApp.iopub_data_rate_limit = 1000000 and change it to a higher default rate. l used c.NotebookApp.iopub_data_rate_limit = 10000000\n",
    "```\n",
    "\n",
    "https://stackoverflow.com/questions/43288550/iopub-data-rate-exceeded-in-jupyter-notebook-when-viewing-image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 100492\n"
     ]
    }
   ],
   "source": [
    "n_patterns = len(x_data)\n",
    "print (\"Total Patterns:\", n_patterns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[0.52941176],\n        [0.02941176],\n        [0.38235294],\n        ...,\n        [0.55882353],\n        [0.17647059],\n        [0.        ]],\n\n       [[0.02941176],\n        [0.38235294],\n        [0.44117647],\n        ...,\n        [0.17647059],\n        [0.        ],\n        [0.47058824]],\n\n       [[0.38235294],\n        [0.44117647],\n        [0.35294118],\n        ...,\n        [0.        ],\n        [0.47058824],\n        [0.02941176]],\n\n       ...,\n\n       [[0.26470588],\n        [0.26470588],\n        [0.29411765],\n        ...,\n        [0.35294118],\n        [0.26470588],\n        [0.32352941]],\n\n       [[0.26470588],\n        [0.29411765],\n        [0.        ],\n        ...,\n        [0.26470588],\n        [0.32352941],\n        [0.44117647]],\n\n       [[0.29411765],\n        [0.        ],\n        [0.52941176],\n        ...,\n        [0.32352941],\n        [0.44117647],\n        [0.41176471]]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = numpy.reshape(x_data, (n_patterns, SEQUENCE_LENGTH, 1))\n",
    "X = X / float(vocab_len)\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np_utils.to_categorical(y_data)\n",
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_12 (LSTM)               (None, 9, 64)             16896     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 9, 64)             0         \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 9, 64)             33024     \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 9, 64)             33024     \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 9, 64)             33024     \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 9, 64)             33024     \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 9, 64)             33024     \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 34)                2210      \n",
      "=================================================================\n",
      "Total params: 217,250\n",
      "Trainable params: 217,250\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(NEURO_ISZE, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(NEURO_ISZE, return_sequences=True))\n",
    "model.add(LSTM(NEURO_ISZE, return_sequences=True))\n",
    "model.add(LSTM(NEURO_ISZE, return_sequences=True))\n",
    "model.add(LSTM(NEURO_ISZE, return_sequences=True))\n",
    "model.add(LSTM(NEURO_ISZE, return_sequences=True))\n",
    "model.add(LSTM(NEURO_ISZE))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "100492/100492 [==============================] - 222s 2ms/step - loss: 3.1389\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.13890, saving model to model_weights_saved.hdf5\n",
      "Epoch 2/4\n",
      "100492/100492 [==============================] - 225s 2ms/step - loss: 3.1218\n",
      "\n",
      "Epoch 00002: loss improved from 3.13890 to 3.12179, saving model to model_weights_saved.hdf5\n",
      "Epoch 3/4\n",
      "100492/100492 [==============================] - 232s 2ms/step - loss: 3.1215\n",
      "\n",
      "Epoch 00003: loss improved from 3.12179 to 3.12146, saving model to model_weights_saved.hdf5\n",
      "Epoch 4/4\n",
      "100492/100492 [==============================] - 251s 2ms/step - loss: 3.1212\n",
      "\n",
      "Epoch 00004: loss improved from 3.12146 to 3.12121, saving model to model_weights_saved.hdf5\n"
     ]
    }
   ],
   "source": [
    "if NEED_TO_FIT_MODEL:\n",
    "    checkpoint = ModelCheckpoint(MODEL_WEIGHTS_PATH, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    desired_callbacks = [checkpoint]\n",
    "    model.fit(X, y, epochs=EPOCH_NUM, batch_size=BATCH_SIZE, callbacks=desired_callbacks)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I've reinstall h5py, because of an error during loading weights\n",
    "```AttributeError: 'str' object has no attribute 'decode'```\n",
    "```bash\n",
    "pip install h5py==2.10.0 --force-reinstall\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "model.load_weights(MODEL_WEIGHTS_PATH)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:\n",
      "[16, 20, 19, 6, 25, 6, 18, 19, 3]\n",
      "путешеств\n"
     ]
    }
   ],
   "source": [
    "start = numpy.random.randint(0, len(x_data) - 1)\n",
    "pattern = x_data[start]\n",
    "print(\"Random Seed:\")\n",
    "print(pattern)\n",
    "print(''.join([num_to_char[value] for value in pattern]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:\t[[0.12744235 0.04126055 0.01702779 0.04760411 0.01921226 0.02991275\n",
      "  0.03535248 0.01482173 0.01767742 0.04371662 0.01254199 0.04195932\n",
      "  0.05523998 0.04256954 0.0654618  0.06206031 0.03534507 0.03536747\n",
      "  0.07051218 0.04500466 0.02420885 0.00448    0.01444068 0.00736378\n",
      "  0.01591341 0.00972368 0.00367934 0.00086351 0.01495632 0.0065235\n",
      "  0.00379237 0.00928333 0.02151926 0.00316161]]\n",
      "Max:\t0.1274423450231552\n",
      "Argmax:\t0\n",
      "Char:\t \n",
      "Prediction:\t[[0.0598829  0.0394749  0.02250754 0.04475373 0.0169071  0.04324585\n",
      "  0.04116851 0.0145909  0.03516687 0.02749087 0.00462094 0.03964369\n",
      "  0.06361637 0.04141466 0.05975288 0.06545603 0.08433738 0.05505801\n",
      "  0.07762568 0.04511007 0.0263584  0.00619349 0.01137932 0.00551456\n",
      "  0.01374922 0.01082249 0.00388751 0.00089075 0.01043459 0.00319752\n",
      "  0.00581302 0.00709635 0.00953626 0.00330168]]\n",
      "Max:\t0.08433738350868225\n",
      "Argmax:\t16\n",
      "Char:\tп\n",
      "Prediction:\t[[0.0338835  0.13441618 0.01559567 0.01849491 0.00848222 0.02143398\n",
      "  0.10082453 0.00601043 0.00827069 0.07998411 0.00309414 0.02218996\n",
      "  0.03472465 0.01620929 0.02671438 0.15762462 0.01078745 0.07022766\n",
      "  0.03066382 0.05942252 0.04324857 0.00126448 0.00375934 0.00289505\n",
      "  0.00831261 0.00474095 0.00587564 0.00031031 0.02006035 0.02126348\n",
      "  0.00060214 0.00978008 0.00947586 0.00935642]]\n",
      "Max:\t0.15762461721897125\n",
      "Argmax:\t15\n",
      "Char:\tо\n"
     ]
    },
    {
     "data": {
      "text/plain": "' пооа оооа оооа оооа оооа оооа'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "\n",
    "for i in range(30):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_len)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "\n",
    "    index = numpy.argmax(prediction)\n",
    "    char = num_to_char[index]\n",
    "    result.append(char)\n",
    "\n",
    "    if i < 3:\n",
    "        print(\n",
    "            f'Prediction:\\t{prediction}',\n",
    "            f'Max:\\t{numpy.max(prediction)}',\n",
    "            f'Argmax:\\t{index}',\n",
    "            f'Char:\\t{char}',\n",
    "            sep='\\n',\n",
    "        )\n",
    "\n",
    "    pattern.append(index)\n",
    "\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "\n",
    "''.join(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}