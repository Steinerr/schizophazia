{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/denisivanov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import numpy\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import (\n",
    "    Dense,\n",
    "    LSTM, Dropout,\n",
    ")\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "TEXT_RESOURCE_PAGES = 10\n",
    "TEXT_CORPUS_PATH = 'text_corpus.txt'\n",
    "MODEL_WEIGHTS_PATH = 'model_weights_saved.hdf5'\n",
    "NEED_TO_LOAD_TEXT = False\n",
    "NEED_TO_FIT_MODEL = False\n",
    "SEQUENCE_LENGTH = 20\n",
    "EPOCH_NUM = 16\n",
    "BATCH_SIZE = 512\n",
    "NEURO_SIZE = 512"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "def _get_domain(url):\n",
    "    return '.'.join(urlparse(url).netloc.split('.')[-2:])\n",
    "\n",
    "\n",
    "def _parse_default(html):\n",
    "    all_texts = ' '.join(re.findall(r'[а-яА-ЯёЁ]+', html))\n",
    "    all_texts = re.sub(r'\\n', r' ', all_texts)\n",
    "    all_texts = re.sub(r'\\s+', ' ', all_texts)\n",
    "    all_texts = all_texts.strip()\n",
    "    return all_texts\n",
    "\n",
    "\n",
    "def _parse_livejournal_page(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    all_texts = soup.find_all('p', {'class': None})\n",
    "    all_texts = ' '.join((p.text for p in all_texts))\n",
    "    return _parse_default(all_texts)\n",
    "\n",
    "\n",
    "page_parsers = {\n",
    "    'livejournal.com': _parse_livejournal_page,\n",
    "    'default': _parse_default,\n",
    "}\n",
    "\n",
    "def get_text_from_url(url) -> str:\n",
    "    print(f'Getting {url} ...')\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f'Some error: {response.status_code}')\n",
    "        return ''\n",
    "\n",
    "    parser = page_parsers.get(_get_domain(url))\n",
    "    if not parser:\n",
    "        parser = page_parsers['default']\n",
    "    return parser(response.text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "if NEED_TO_LOAD_TEXT:\n",
    "    urls = [f'https://pesen-net.livejournal.com/?skip={i*10}' for i in range(TEXT_RESOURCE_PAGES)]\n",
    "\n",
    "    texts = []\n",
    "    for url in urls:\n",
    "        text = get_text_from_url(url)\n",
    "        texts.append(text)\n",
    "\n",
    "    text = ' '.join(texts)\n",
    "    cleaned_text = text.lower()\n",
    "\n",
    "    with open(TEXT_CORPUS_PATH, 'w') as f:\n",
    "        f.write(cleaned_text)\n",
    "\n",
    "else:\n",
    "    with open(TEXT_CORPUS_PATH, 'r') as f:\n",
    "        cleaned_text = f.read()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(cleaned_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "cleaned_text = ' '.join(filter(lambda x: x not in stopwords.words('russian'), tokens))\n",
    "chars = sorted(list(set(cleaned_text)))\n",
    "char_to_num = {char: num for num, char in enumerate(chars)}\n",
    "num_to_char = {num: char for num, char in enumerate(chars)}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "input_len = len(cleaned_text)\n",
    "vocab_len = len(chars)\n",
    "x_data, y_data = [], []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In seq:\tсамолёте пассажир со\n",
      "Out seq:\tб\n",
      "X data:\t[[18, 1, 13, 15, 12, 33, 19, 6, 0, 16, 1, 18, 18, 1, 7, 9, 17, 0, 18, 15]]\n",
      "Y data: \t[2]\n",
      "In seq:\tамолёте пассажир соб\n",
      "Out seq:\tл\n",
      "X data:\t[[18, 1, 13, 15, 12, 33, 19, 6, 0, 16, 1, 18, 18, 1, 7, 9, 17, 0, 18, 15], [1, 13, 15, 12, 33, 19, 6, 0, 16, 1, 18, 18, 1, 7, 9, 17, 0, 18, 15, 2]]\n",
      "Y data: \t[2, 12]\n",
      "In seq:\tмолёте пассажир собл\n",
      "Out seq:\tа\n",
      "X data:\t[[18, 1, 13, 15, 12, 33, 19, 6, 0, 16, 1, 18, 18, 1, 7, 9, 17, 0, 18, 15], [1, 13, 15, 12, 33, 19, 6, 0, 16, 1, 18, 18, 1, 7, 9, 17, 0, 18, 15, 2], [13, 15, 12, 33, 19, 6, 0, 16, 1, 18, 18, 1, 7, 9, 17, 0, 18, 15, 2, 12]]\n",
      "Y data: \t[2, 12, 1]\n",
      "In seq:\tолёте пассажир собла\n",
      "Out seq:\tз\n",
      "X data:\t[[18, 1, 13, 15, 12, 33, 19, 6, 0, 16, 1, 18, 18, 1, 7, 9, 17, 0, 18, 15], [1, 13, 15, 12, 33, 19, 6, 0, 16, 1, 18, 18, 1, 7, 9, 17, 0, 18, 15, 2], [13, 15, 12, 33, 19, 6, 0, 16, 1, 18, 18, 1, 7, 9, 17, 0, 18, 15, 2, 12], [15, 12, 33, 19, 6, 0, 16, 1, 18, 18, 1, 7, 9, 17, 0, 18, 15, 2, 12, 1]]\n",
      "Y data: \t[2, 12, 1, 8]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, input_len - SEQUENCE_LENGTH, 1):\n",
    "    in_seq = cleaned_text[i:i + SEQUENCE_LENGTH]\n",
    "    out_seq = cleaned_text[i + SEQUENCE_LENGTH]\n",
    "\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])\n",
    "\n",
    "    if i <= 3:\n",
    "        print(\n",
    "            f'In seq:\\t{in_seq}',\n",
    "            f'Out seq:\\t{out_seq}',\n",
    "            f'X data:\\t{x_data}',\n",
    "            f'Y data: \\t{y_data}',\n",
    "            sep='\\n'\n",
    "        )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If  you want to print something from `for`:\n",
    "\n",
    "```\n",
    "IOPub data rate exceeded.\n",
    "The notebook server will temporarily stop sending output\n",
    "to the client in order to avoid crashing it.\n",
    "To change this limit, set the config variable\n",
    "`--NotebookApp.iopub_data_rate_limit`.\n",
    "\n",
    "Current values:\n",
    "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
    "NotebookApp.rate_limit_window=3.0 (secs)\n",
    "```\n",
    "\n",
    "```\n",
    "To create a jupyter_notebook_config.py file, with all the defaults commented out, you can use the following command line:\n",
    "\n",
    "$ jupyter notebook --generate-config\n",
    "\n",
    "Open the file and search for c.NotebookApp.iopub_data_rate_limit\n",
    "\n",
    "Comment out the line c.NotebookApp.iopub_data_rate_limit = 1000000 and change it to a higher default rate. l used c.NotebookApp.iopub_data_rate_limit = 10000000\n",
    "```\n",
    "\n",
    "https://stackoverflow.com/questions/43288550/iopub-data-rate-exceeded-in-jupyter-notebook-when-viewing-image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 100481\n"
     ]
    }
   ],
   "source": [
    "n_patterns = len(x_data)\n",
    "print (\"Total Patterns:\", n_patterns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[0.52941176],\n        [0.02941176],\n        [0.38235294],\n        ...,\n        [0.        ],\n        [0.52941176],\n        [0.44117647]],\n\n       [[0.02941176],\n        [0.38235294],\n        [0.44117647],\n        ...,\n        [0.52941176],\n        [0.44117647],\n        [0.05882353]],\n\n       [[0.38235294],\n        [0.44117647],\n        [0.35294118],\n        ...,\n        [0.44117647],\n        [0.05882353],\n        [0.35294118]],\n\n       ...,\n\n       [[0.44117647],\n        [0.47058824],\n        [0.58823529],\n        ...,\n        [0.35294118],\n        [0.26470588],\n        [0.32352941]],\n\n       [[0.47058824],\n        [0.58823529],\n        [0.64705882],\n        ...,\n        [0.26470588],\n        [0.32352941],\n        [0.44117647]],\n\n       [[0.58823529],\n        [0.64705882],\n        [0.        ],\n        ...,\n        [0.32352941],\n        [0.44117647],\n        [0.41176471]]])"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = numpy.reshape(x_data, (n_patterns, SEQUENCE_LENGTH, 1))\n",
    "X = X / float(vocab_len)\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 1., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32)"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np_utils.to_categorical(y_data)\n",
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_22 (LSTM)               (None, 20, 512)           1052672   \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 20, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_23 (LSTM)               (None, 20, 512)           2099200   \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 20, 512)           0         \n",
      "_________________________________________________________________\n",
      "lstm_24 (LSTM)               (None, 256)               787456    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 34)                8738      \n",
      "=================================================================\n",
      "Total params: 3,948,066\n",
      "Trainable params: 3,948,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(NEURO_SIZE, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(NEURO_SIZE, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(int(NEURO_SIZE / 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [],
   "source": [
    "if NEED_TO_FIT_MODEL:\n",
    "    checkpoint = ModelCheckpoint(MODEL_WEIGHTS_PATH, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    desired_callbacks = [checkpoint]\n",
    "    model.fit(X, y, epochs=EPOCH_NUM, batch_size=BATCH_SIZE, callbacks=desired_callbacks)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I've reinstall h5py, because of an error during loading weights\n",
    "```AttributeError: 'str' object has no attribute 'decode'```\n",
    "```bash\n",
    "pip install h5py==2.10.0 --force-reinstall\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "outputs": [],
   "source": [
    "model.load_weights(MODEL_WEIGHTS_PATH)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:\n",
      "[17, 4, 13, 11, 22, 16, 0, 19, 22, 19, 22, 15, 8, 9, 8, 31, 19, 10, 0, 10]\n",
      "ргмкхп тхтхозизютй й\n",
      "Prediction:\t[[3.7754485e-03 2.9628980e-01 1.0279366e-02 1.8305356e-02 1.9361570e-02\n",
      "  4.1098166e-02 5.7384878e-02 1.1294904e-03 2.5825379e-02 3.9272014e-02\n",
      "  2.8626146e-04 1.3105953e-03 2.0950457e-02 1.2283113e-02 1.4278574e-01\n",
      "  1.1952854e-01 1.4679553e-03 7.3389836e-02 2.8962744e-02 6.3163107e-03\n",
      "  4.1250158e-02 5.5246078e-04 2.1606458e-03 1.3026056e-03 8.4159913e-04\n",
      "  1.0998097e-03 3.0364662e-03 1.8357257e-04 6.4738379e-03 4.1809585e-04\n",
      "  6.4184674e-04 1.1243784e-02 1.9141474e-03 8.8779386e-03]]\n",
      "Max:\t0.2962898015975952\n",
      "Argmax:\t1\n",
      "Char:\tа\n"
     ]
    },
    {
     "data": {
      "text/plain": "'арантине собак  саздели собака сазвилась селефон пододненно сазвитель сакон поддотова саидели пододроиненно пододненно сазвитель сакон поддотова саидели пододроиненно пододненно сазвитель сакон поддотова саидели пододроиненно пододненно сазвитель сакон поддотова саидели пододроиненно пододненно сазв'"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars_indexes = []\n",
    "for i in range(SEQUENCE_LENGTH):\n",
    "    char_index = numpy.random.randint(0, vocab_len - 1)\n",
    "    chars_indexes.append(char_index)\n",
    "\n",
    "pattern = chars_indexes\n",
    "# pattern = x_data[start]\n",
    "print(\"Random Seed:\")\n",
    "print(pattern)\n",
    "print(''.join([num_to_char[value] for value in pattern]), '\\n\\n')\n",
    "\n",
    "result = []\n",
    "\n",
    "for i in range(300):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_len)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "\n",
    "    index = numpy.argmax(prediction)\n",
    "    char = num_to_char[index]\n",
    "    result.append(char)\n",
    "\n",
    "    if i < 1:\n",
    "        print(\n",
    "            f'Prediction:\\t{prediction}',\n",
    "            f'Max:\\t{numpy.max(prediction)}',\n",
    "            f'Argmax:\\t{index}',\n",
    "            f'Char:\\t{char}',\n",
    "            sep='\\n',\n",
    "        )\n",
    "\n",
    "    pattern.append(index)\n",
    "\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "\n",
    "''.join(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}